import streamlit as st
import os 
import json
import io
import docx2txt
import re
import nltk
import pymorphy2
from dotenv import dotenv_values
from fuzzywuzzy import fuzz
from fuzzywuzzy import process 
import pandas as pd

morph = pymorphy2.MorphAnalyzer()

def get_doc_text(filepath, file):
    if True:#file.endswith('.docx'):
        text = docx2txt.process(filepath+file)
        return text
    elif file.endswith('.doc'):
        # converting .doc to .docx
        doc_file = filepath + file
        docx_file = filepath + file + 'x'
        if not os.path.exists(docx_file):
            os.system('antiword ' + doc_file + ' > ' + docx_file)
            with open(docx_file) as f:
                text = f.read()
            os.remove(docx_file) #docx_file was just to read, so deleting
        else:
          # already a file with same name as doc exists having docx extension, 
          # which means it is a different file, so we cant read it
            print('Info : file with same name of doc exists having docx extension, so we cant read it')
            text = ''
        return text
    
def clean_text(text, clean_PD=False):
    text = re.sub('[\\n]{2,99}','\n',text)
    text = re.sub(r'[\s]{2,99}',' ',text)
    if clean_PD:
        text = re.sub('[#‚Ññ:\s]{1}[\d]{15}\s',' 555555555555555 ',text) #–ï–≥—Ä–∏–ø
        text = re.sub('[#‚Ññ:\s]{1}[\d]{13}\s',' 5555555555555 ',text) #–û–ì–†–ù
        text = re.sub('[#‚Ññ:\s]{1}[\d]{12}\s',' 555555555555 ',text) #–ò–ù–ù —Ñ–∏–∑ –ª–∏—Ü
        text = re.sub('[#‚Ññ:\s]{1}[\d]{10}\s',' 5555555555 ',text) #–ò–ù–ù —é—Ä –ª–∏—Ü
        text = re.sub('[#‚Ññ:\s]{1}[\d]{9}\s', ' 555555555 ',text) #–ë–ò–ö
        text = re.sub('[#‚Ññ:\s]{1}[\d]{20}\s',' 55555555555555555555 ',text) #—Å—á–µ—Ç–∞
        text = re.sub('((8|\+7)[\- ]?){1}(\(?\d{3}\)?[\- ]?)?[\d\- ]{7,10}',' +74955555555 ',text)
        text = re.sub('[A-Za-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,4}',' admin@mail.ru ',text)

        prob_thresh = 0.07
        fio = {}
        for word in nltk.word_tokenize(text):
            if word[-1]=='.':
                word = word[:-1]
            for p in morph.parse(word):
                if word in ('–ï–ì–†–ò–ü', '–ò–ù–ù', '–û–ì–†–ù–ò–ü'):
                    continue
                if 'Name' in p.tag and p.score >= prob_thresh  and word not in fio.keys():
                    print('Name', word, p.score)
                    fio[word] = morph.parse('–ò–≤–∞–Ω')[0].inflect({p.tag.case}).word.title()
                elif 'Patr' in p.tag and p.score >= prob_thresh  and word not in fio.keys():
                    print('Patr', word, p.score)
                    fio[word] = morph.parse('–ò–≤–∞–Ω–æ–≤–∏—á')[0].inflect({p.tag.case}).word.title()
                elif 'Surn' in p.tag and p.score >= prob_thresh and word not in fio.keys():
                    print('Surn', word, p.score)
                    fio[word] = morph.parse('–ò–≤–∞–Ω–æ–≤')[0].inflect({p.tag.case}).word.title()
        #         elif 'Geox' in p.tag and p.score >= prob_thresh and word not in fio.keys():
        #             print('Geos', word, p.score)
        #             fio[word] = morph.parse('–ë–æ–±—Ä—É–π—Å–∫')[0].inflect({p.tag.case}).word.title()

        for w ,n in fio.items():
            text = text.replace(w, n)
    return text

st.set_page_config(page_title="–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –ø–æ —á–µ–∫–ª–∏—Å—Ç—É", page_icon="üìù")
st.title('–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –ø–æ —á–µ–∫–ª–∏—Å—Ç—É')
clean_PD = st.sidebar.checkbox('–°–∫—Ä—ã–≤–∞—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ', False)
uploaded_file = st.sidebar.file_uploader('–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–æ–≥–æ–≤–æ—Ä–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏...', type={ "docx"})


if uploaded_file:
    # uploaded_file = StringIO(uploaded_file.getvalue().decode("utf-8"))
    # with open(uploaded_file, 'r') as f:
    #     dialog = f.read()
    bytes_data = uploaded_file.getvalue()
    raw_text = docx2txt.process(uploaded_file)
    # st.write("dogovor.docx", uploaded_file.name)
    # st.write(bytes_data)
    # text = get_doc_text("", "dogovor.docx")
    text = clean_text(raw_text, clean_PD=clean_PD)
    # file = uploaded_file.read()#.decode('utf-8')
    st.text_area('–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –î–æ–≥–æ–≤–æ—Ä–∞',value=text)
    # st.write('–ü–î –Ω–µ —É–¥–∞–ª—è—é—Ç—Å—è')
    df = pd.read_excel('dogovor_checklist.xlsx')
    df['–û–ø–∏—Å–∞–Ω–∏–µ'] = df['–û–ø–∏—Å–∞–Ω–∏–µ'].ffill()
    result = []
    output = ""
    for p in df['–û–ø–∏—Å–∞–Ω–∏–µ'].unique():
        output = output + f"\n{p},(–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –ø—É–Ω–∫—Ç–æ–≤ {len(df[df['–û–ø–∏—Å–∞–Ω–∏–µ']==p]['–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ '].tolist())}) \n"
        for i, f in enumerate(df[df['–û–ø–∏—Å–∞–Ω–∏–µ']==p]['–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ '].tolist()):
            if pd.isnull(f):
                continue
            r = fuzz.partial_ratio(f,text[6:])    # –Ω–µ –ø–æ–Ω–∏–º–∞—é —á—Ç–æ –∑–∞ –∫–æ—Å—è–∫ —Å –ø–æ–ª–Ω–æ–π —à–∞–ø–∫–æ–π, –∫–∞–∂–µ—Ç—Å—è –∫–∞–∫–æ–π-—Ç–æ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª –ø—Ä–æ—Å–∫–∞–∫–∏–≤–∞–µ—Ç
            if r==45:
                st.write(f)
            if r>=90:
                givenOpt = text.split('\n') 
                ratios = process.extract(f,givenOpt) 
                output = output + f' - –ü—É–Ω–∫—Ç {i+1} –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {ratios[0][0]}\n'
                res = '–ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç'
            else:
                output = output + f' - –ü—É–Ω–∫—Ç {i+1} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –ë–ª–∏–∂–∞–π—à–∏–π: {r} %\n'
                givenOpt = text.split('\n') 
                ratios = process.extract(f,givenOpt) 
                res = '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'
            result.append([p,f,res, ratios[0][0]])    
    dr = pd.DataFrame(result, columns=['–†–∞–∑–¥–µ–ª', '–ò—Å–∫–æ–º—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç', '–†–µ–∑—É–ª—å—Ç–∞—Ç', '–ù–∞–π–¥–µ–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç'])
    dr['–ù–∞–π–¥–µ–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç'] = dr.apply(lambda x: None if x['–†–µ–∑—É–ª—å—Ç–∞—Ç']=='–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç' else x['–ù–∞–π–¥–µ–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç'],axis=1)

    st.write(f"""–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –ø—É–Ω–∫—Ç–æ–≤: {len(dr)}  
–ù–∞–π–¥–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(dr[dr['–†–µ–∑—É–ª—å—Ç–∞—Ç']=='–ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç'])}  
–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(dr[dr['–†–µ–∑—É–ª—å—Ç–∞—Ç']=='–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'])}  
–ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –≤ Excel""")
    buffer = io.BytesIO()
    # dr.to_excel(buffer, sheet_name='Sheet1', index=False)
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
    # Write each dataframe to a different worksheet.
        dr.to_excel(writer, sheet_name='Sheet1', index=False)
        worksheet = writer.sheets['Sheet1']  # pull worksheet object

        for idx, col in enumerate(dr):  # loop through all columns
            series = dr[col]
            max_len = max((
                series.astype(str).map(len).max(),  # len of largest item
                len(str(series.name))  # len of column name/header
                )) + 1  # adding a little extra space
            print(max_len)
            if idx==0:
                worksheet.set_column(idx, idx, 25)
            elif max_len>100:
                worksheet.set_column(idx, idx, 100)
            else: 
                worksheet.set_column(idx, idx, max_len)  # set column width
        # writer.close()

    download2 = st.download_button(
        label="–°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç –æ –ø—Ä–æ–≤–µ—Ä–∫–µ –≤ Excel",
        data=buffer,
        key=1,
        file_name=f'{uploaded_file.name}_checkResults.xlsx',
        mime='application/vnd.ms-excel'
    )
    # print(output)

    st.text_area('–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏: ', output, height=400)
#             print(ratios[0][0])
    # st.text_area('–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏–∞–ª–æ–≥–∞',value=dialog)
    # with st.spinner(text="–ñ–¥—ë–º –æ—Ç–≤–µ—Ç–∞ –æ—Ç YandexGPT, –Ω–µ–º–Ω–æ–≥–æ —Ç–µ—Ä–ø–µ–Ω–∏—è..."):
    #     ans, answer, json_valid = ask_gpt(dialog)

    # try:
    #     df = pd.DataFrame(pd.Series(answer), columns=['–†–µ–∑—É–ª—å—Ç–∞—Ç'])
    #     df.index.name = '–í–æ–ø—Ä–æ—Å—ã'
    #     st.dataframe(df.reset_index(), hide_index=True)
    # except Exception as e:
    #     st.text_area('–ù–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–æ—Å—å',value=f'{e}\n\n{e.args}')

    # st.text_area('–û—Ç–≤–µ—Ç –Ø–Ω–¥–µ–∫—Å–∞',value=f'{json_valid}\n\n{answer}', height=300)
    # st.text_area('–û—Ç–≤–µ—Ç –Ø–Ω–¥–µ–∫—Å–∞ –ø–æ–ª–Ω—ã–π',value=f'{ans}\n\n{json_valid}', height=300)

